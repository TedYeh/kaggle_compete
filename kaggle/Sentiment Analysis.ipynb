{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaModel, BertModel, AutoModel\n",
    "from transformers import RobertaTokenizer, BertTokenizer, AutoTokenizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: TITAN X (Pascal)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "import requests\n",
    "request = requests.get(\"https://drive.google.com/uc?export=download&id=1wHt8PsMLsfX5yNSqrt2fSTcb8LEiclcf\")\n",
    "with open(\"data.zip\", \"wb\") as file:\n",
    "    file.write(request.content)\n",
    "\n",
    "# Unzip data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('data.zip') as zip:\n",
    "    zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>15321</td>\n",
       "      <td>Good job @amtrak. No wifi on 8:10 train NYC to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>73915</td>\n",
       "      <td>@AlaskaAir shaniqwa in reservations was so hel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>126062</td>\n",
       "      <td>@SouthwestAir is it better to purchase the ear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>16045</td>\n",
       "      <td>@DeltaAssist so I got home at 12:30 after the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>69246</td>\n",
       "      <td>@AmericanAir it has now been over 10 days sinc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  label\n",
       "461    15321  Good job @amtrak. No wifi on 8:10 train NYC to...      0\n",
       "2489   73915  @AlaskaAir shaniqwa in reservations was so hel...      1\n",
       "2968  126062  @SouthwestAir is it better to purchase the ear...      1\n",
       "865    16045  @DeltaAssist so I got home at 12:30 after the ...      0\n",
       "1418   69246  @AmericanAir it has now been over 10 days sinc...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and set labels\n",
    "data_complaint = pd.read_csv('data/complaint1700.csv')\n",
    "data_complaint['label'] = 0\n",
    "data_non_complaint = pd.read_csv('data/noncomplaint1700.csv')\n",
    "data_non_complaint['label'] = 1\n",
    "\n",
    "# Concatenate complaining and non-complaining data\n",
    "data = pd.concat([data_complaint, data_non_complaint], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Drop 'airline' column\n",
    "data.drop(['airline'], inplace=True, axis=1)\n",
    "\n",
    "# Display 5 random samples\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.tweet.values\n",
    "y = data.label.values\n",
    "\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>85233</td>\n",
       "      <td>@KayCockerill @AmericanAir @sfo Brutal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>49697</td>\n",
       "      <td>You know what's a total fucking blast?  Sittin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>111006</td>\n",
       "      <td>@DeltaAssist always comes through with the bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>77011</td>\n",
       "      <td>@AaronCarpenter @AmericanAir They suck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>292</td>\n",
       "      <td>@SouthwestAir flight 195 delayed. So much for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet\n",
       "2207   85233            @KayCockerill @AmericanAir @sfo Brutal.\n",
       "1268   49697  You know what's a total fucking blast?  Sittin...\n",
       "2920  111006  @DeltaAssist always comes through with the bes...\n",
       "1996   77011             @AaronCarpenter @AmericanAir They suck\n",
       "8        292  @SouthwestAir flight 195 delayed. So much for ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "# Keep important columns\n",
    "test_data = test_data[['id', 'tweet']]\n",
    "\n",
    "# Display 5 samples from the test data\n",
    "test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def get_auc_CV(model):\n",
    "    \"\"\"\n",
    "    Return the average AUC score from cross-validation.\n",
    "    \"\"\"\n",
    "    # Set KFold to shuffle data before the split\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    # Get AUC scores\n",
    "    auc = cross_val_score(\n",
    "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    \n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "Original:  @united I'm having issues. Yesterday I rebooked for 24 hours after I was supposed to fly, now I can't log on &amp; check in. Can you help?\n",
      "Processed:  I'm having issues. Yesterday I rebooked for 24 hours after I was supposed to fly, now I can't log on & check in. Can you help?\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0\n",
    "print(len(X[0]))\n",
    "print('Original: ', X[0])\n",
    "print('Processed: ', text_preprocessing(X[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            #truncation=True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  68\n"
     ]
    }
   ],
   "source": [
    "# Concatenate train data and test data\n",
    "all_tweets = np.concatenate([data.tweet.values, test_data.tweet.values])\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  @united kinda feel like the $6.99 you charge for in flight Wi-Fi is ridiculous. AND it sucks, slow, or doesn't work. #anythingtomakeabuck\n",
      "Token IDs:  tensor([[  101, 17704,  2514,  2066,  1996,  1002,  1020,  1012,  5585,  2017,\n",
      "          3715,  2005,  1999,  3462, 15536,  1011, 10882,  2003,  9951,  1012,\n",
      "          1998,  2009, 19237,  1010,  4030,  1010,  2030,  2987,  1005,  1056,\n",
      "          2147,  1012,  1001,  2505, 20389, 13808,  7875, 12722,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids, tmp_mask = preprocessing_for_bert([X[1]])[0], preprocessing_for_bert([X[1]])[1]\n",
    "print('Original: ', X[1])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(in_features, hidden_dim)\n",
    "        self.W2 = nn.Linear(in_features, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        query, key = self.W1(features), self.W2(features)\n",
    "        att = torch.tanh(query + key)\n",
    "        score = self.V(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 64, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        #self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        #self.bert = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "    \n",
    "        self.head = AttentionHead(D_in, D_in, 1)\n",
    "        \n",
    "        self.linear = nn.Linear(D_in, D_out)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        all_hidden_state = torch.stack(outputs[2])\n",
    "        #print(all_hidden_state[-3:].shape)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        last_hidden_state = outputs[0]\n",
    "        '''\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        \n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        '''        \n",
    "        #outputs = self.head(outputs[0])\n",
    "        outputs = self.head(torch.sum(all_hidden_state[-4:] ,0))\n",
    "        outputs = self.dropout(outputs)\n",
    "        logits = self.linear(outputs)        \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2993, -1.5673]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model = BertClassifier()\n",
    "tmp_output = tmp_model(token_ids, tmp_mask)\n",
    "tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            \n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.799242   |     -      |     -     |   7.71   \n",
      "   1    |   40    |   0.552821   |     -      |     -     |   7.31   \n",
      "   1    |   60    |   0.522342   |     -      |     -     |   7.37   \n",
      "   1    |   80    |   0.508902   |     -      |     -     |   7.41   \n",
      "   1    |   95    |   0.475507   |     -      |     -     |   5.47   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.579146   |  0.477559  |   74.89   |   36.55  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.299143   |     -      |     -     |   7.99   \n",
      "   2    |   40    |   0.280003   |     -      |     -     |   7.72   \n",
      "   2    |   60    |   0.293488   |     -      |     -     |   8.21   \n",
      "   2    |   80    |   0.259025   |     -      |     -     |   8.18   \n",
      "   2    |   95    |   0.256354   |     -      |     -     |   6.01   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.278934   |  0.561319  |   77.61   |   39.49  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.104870   |     -      |     -     |   8.60   \n",
      "   3    |   40    |   0.084774   |     -      |     -     |   8.18   \n",
      "   3    |   60    |   0.114794   |     -      |     -     |   8.22   \n",
      "   3    |   80    |   0.049824   |     -      |     -     |   7.98   \n",
      "   3    |   95    |   0.123045   |     -      |     -     |   5.98   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.094123   |  0.892843  |   76.76   |   40.36  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   20    |   0.029305   |     -      |     -     |   8.57   \n",
      "   4    |   40    |   0.019370   |     -      |     -     |   8.17   \n",
      "   4    |   60    |   0.013430   |     -      |     -     |   8.20   \n",
      "   4    |   80    |   0.045622   |     -      |     -     |   8.19   \n",
      "   4    |   95    |   0.014304   |     -      |     -     |   5.98   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.024983   |  1.081163  |   78.07   |   40.49  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   20    |   0.003387   |     -      |     -     |   8.43   \n",
      "   5    |   40    |   0.005064   |     -      |     -     |   8.16   \n",
      "   5    |   60    |   0.003497   |     -      |     -     |   7.80   \n",
      "   5    |   80    |   0.004737   |     -      |     -     |   7.89   \n",
      "   5    |   95    |   0.018354   |     -      |     -     |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.006379   |  1.252974  |   78.35   |   39.33  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=5)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=5, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8644\n",
      "Accuracy: 77.94%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.738165   |     -      |     -     |   7.84   \n",
      "   1    |   40    |   0.552243   |     -      |     -     |   7.57   \n",
      "   1    |   60    |   0.506132   |     -      |     -     |   7.92   \n",
      "   1    |   80    |   0.510420   |     -      |     -     |   8.20   \n",
      "   1    |   100   |   0.467208   |     -      |     -     |   8.19   \n",
      "   1    |   106   |   0.416898   |     -      |     -     |   2.19   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.248962   |     -      |     -     |   8.46   \n",
      "   2    |   40    |   0.271566   |     -      |     -     |   8.17   \n",
      "   2    |   60    |   0.254362   |     -      |     -     |   8.21   \n",
      "   2    |   80    |   0.219578   |     -      |     -     |   8.19   \n",
      "   2    |   100   |   0.262437   |     -      |     -     |   8.20   \n",
      "   2    |   106   |   0.202322   |     -      |     -     |   2.21   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.075323   |     -      |     -     |   8.45   \n",
      "   3    |   40    |   0.059479   |     -      |     -     |   8.21   \n",
      "   3    |   60    |   0.070457   |     -      |     -     |   8.20   \n",
      "   3    |   80    |   0.083965   |     -      |     -     |   8.18   \n",
      "   3    |   100   |   0.075065   |     -      |     -     |   8.17   \n",
      "   3    |   106   |   0.042460   |     -      |     -     |   2.19   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   20    |   0.020852   |     -      |     -     |   8.46   \n",
      "   4    |   40    |   0.009518   |     -      |     -     |   7.98   \n",
      "   4    |   60    |   0.018362   |     -      |     -     |   7.92   \n",
      "   4    |   80    |   0.029959   |     -      |     -     |   7.84   \n",
      "   4    |   100   |   0.022863   |     -      |     -     |   7.97   \n",
      "   4    |   106   |   0.056815   |     -      |     -     |   2.10   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   20    |   0.004529   |     -      |     -     |   8.34   \n",
      "   5    |   40    |   0.003631   |     -      |     -     |   8.01   \n",
      "   5    |   60    |   0.011400   |     -      |     -     |   7.93   \n",
      "   5    |   80    |   0.007369   |     -      |     -     |   7.81   \n",
      "   5    |   100   |   0.004592   |     -      |     -     |   7.85   \n",
      "   5    |   106   |   0.018427   |     -      |     -     |   2.13   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the train set and the validation set\n",
    "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "full_train_sampler = RandomSampler(full_train_data)\n",
    "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n",
    "\n",
    "# Train the Bert Classifier on the entire training data\n",
    "set_seed(42)\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=5)\n",
    "train(bert_classifier, full_train_dataloader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islab\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2111: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(test_data.tweet)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets predicted non-negative:  630\n"
     ]
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "# Get predictions from the probabilities\n",
    "threshold = 0.992\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of tweets predicted non-negative: \", preds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda",
   "language": "python",
   "name": "pytorch_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
